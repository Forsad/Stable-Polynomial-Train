{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath('../src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import wandb\n",
    "key = None\n",
    "if key is not None:\n",
    "    wandb.login(key=key)\n",
    "    os.environ['WANDB_API_KEY'] = key\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"uci_har.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 23 10:28:41 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   32C    P0             41W /  300W |       1MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_151599/213457286.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train = pd.read_csv(fol + 'UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None)\n",
      "/tmp/ipykernel_151599/213457286.py:10: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_test = pd.read_csv(fol + 'UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None)\n",
      "/tmp/ipykernel_151599/213457286.py:14: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  features = pd.read_csv(fol + 'UCI HAR Dataset/features.txt', header=None, delim_whitespace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training data\n",
    "fol = \"data/uci_har/\"\n",
    "X_train = pd.read_csv(fol + 'UCI HAR Dataset/train/X_train.txt', delim_whitespace=True, header=None)\n",
    "y_train = pd.read_csv(fol + 'UCI HAR Dataset/train/y_train.txt', header=None, names=['activity'])\n",
    "subject_train = pd.read_csv(fol + 'UCI HAR Dataset/train/subject_train.txt', header=None, names=['subject'])\n",
    "y_train -= 1\n",
    "# Load test data\n",
    "X_test = pd.read_csv(fol + 'UCI HAR Dataset/test/X_test.txt', delim_whitespace=True, header=None)\n",
    "y_test = pd.read_csv(fol + 'UCI HAR Dataset/test/y_test.txt', header=None, names=['activity'])\n",
    "subject_test = pd.read_csv(fol + 'UCI HAR Dataset/test/subject_test.txt', header=None, names=['subject'])\n",
    "y_test -= 1\n",
    "features = pd.read_csv(fol + 'UCI HAR Dataset/features.txt', header=None, delim_whitespace=True)\n",
    "X_train.columns = features[1].values\n",
    "X_test.columns = features[1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 1)\n"
     ]
    }
   ],
   "source": [
    "print(subject_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 561) (7352, 1) (7352, 1)\n",
      "(2947, 561) (2947, 1) (2947, 1)\n",
      "[0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(X_train.shape, y_train.shape, subject_train.shape)\n",
    "print(X_test.shape, y_test.shape, subject_test.shape)\n",
    "print(np.unique(y_train.to_numpy().flatten()))\n",
    "reverse_mp = {0: 'WALKING', 1: 'WALKING_UPSTAIRS', 2: 'WALKING_DOWNSTAIRS', 3: 'SITTING', 4: 'STANDING', 5: 'LAYING'}\n",
    "mp = {y: i for i, y in reverse_mp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6220,) (1132,)\n"
     ]
    }
   ],
   "source": [
    "# Example data: X (features), y (labels), groups (subject IDs)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "splitter = GroupShuffleSplit(test_size=0.1, n_splits=1, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(X_train, y_train, groups=subject_train))\n",
    "print(train_idx.shape, val_idx.shape)\n",
    "X_val = X_train.iloc[val_idx, :]\n",
    "X_train = X_train.iloc[train_idx, :]\n",
    "y_val = y_train.iloc[val_idx]\n",
    "y_train = y_train.iloc[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6220, 561) (6220, 1) (1132, 561) (1132, 1) (2947, 561) (2947, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(561,) (561,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def normalize_data(X):\n",
    "    X_mean = X.mean(axis=0)\n",
    "    X_std = X.std(axis=0)\n",
    "    return (X - X_mean) / X_std, X_mean, X_std\n",
    "\n",
    "X_train, X_train_mean, X_train_std = normalize_data(X_train)\n",
    "print(X_train_mean.shape, X_train_std.shape)\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "X_val = (X_val - X_train_mean) / X_train_std\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(torch.from_numpy(X_train.to_numpy()).float(), torch.from_numpy(y_train.to_numpy().flatten()).long())\n",
    "test_set = torch.utils.data.TensorDataset(torch.from_numpy(X_test.to_numpy()).float(), torch.from_numpy(y_test.to_numpy().flatten()).long())\n",
    "val_set = torch.utils.data.TensorDataset(torch.from_numpy(X_val.to_numpy()).float(), torch.from_numpy(y_val.to_numpy().flatten()).long())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6220 2947 1132\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set), len(test_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 13 samp_size 100 initialization least_square pol_degree 2\n",
      "B 13 samp_size 100 initialization least_square pol_degree 2\n",
      "B 13 samp_size 100 initialization least_square pol_degree 2\n",
      "B 13 samp_size 100 initialization least_square pol_degree 2\n",
      "B 13 samp_size 100 initialization least_square pol_degree 2\n",
      "B 13 samp_size 100 initialization least_square pol_degree 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/conda/envs/hfe/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/opt/conda/envs/hfe/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name           | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model          | MLP              | 190 K  | train\n",
      "1 | base_criterion | CrossEntropyLoss | 0      | train\n",
      "2 | criterion      | CustomPolyLoss   | 0      | train\n",
      "------------------------------------------------------------\n",
      "190 K     Trainable params\n",
      "0         Non-trainable params\n",
      "190 K     Total params\n",
      "0.760     Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb05adc4e72c4d2db89a2175622b557e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hfe/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a521043226c6437a9254f40747532f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97c4fcb24594a69868d809e0bcce1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7eb6e9e72240ff9cddbe37c04b06d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01234834c6554662ae7cb07aeaaf6ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8031520f08b4aa3af5ba70d8a3c9ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2db11832855406fb51d605b811fc316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fd8d28185a4edebf61320b0249c37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acc106908384e92ace719a57bf5777a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef38c2f42fb4cbc851ce101aeda56aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36bf6ba723e2446e98adbbed9c67b42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b6781a6e7b4c11bf949c2da775fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "Restoring states from the checkpoint path at /code/Polynomial-NN/notebooks/lightning_logs/version_60/checkpoints/best-acc-epoch=8-val_acc=0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /code/Polynomial-NN/notebooks/lightning_logs/version_60/checkpoints/best-acc-epoch=8-val_acc=0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e29356517e04feb83ff96d4a7d2251b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /code/Polynomial-NN/notebooks/lightning_logs/version_60/checkpoints/best-acc-epoch=8-val_acc=0.00.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /code/Polynomial-NN/notebooks/lightning_logs/version_60/checkpoints/best-acc-epoch=8-val_acc=0.00.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Validate metric               DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       val_acc_epoch             0.9805653691291809\n",
      "  val_boundary_loss_epoch               0.0\n",
      "val_cross_entropy_loss_epoch     0.3543056845664978\n",
      "       val_loss_epoch            0.3543056845664978\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0e9ec77e9f44d0ba6fd56b6a3f8fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      Validate metric               DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       val_acc_epoch             0.9402782320976257\n",
      "  val_boundary_loss_epoch               0.0\n",
      "val_cross_entropy_loss_epoch     0.4108952283859253\n",
      "       val_loss_epoch            0.4108952283859253\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from run_experiment import run_expriment\n",
    "import random\n",
    "import string\n",
    "\n",
    "#Comment this lines for relu\n",
    "\n",
    "\n",
    "# activation = 'relu'\n",
    "# enable_boundary_loss = False\n",
    "# disable_batchnorm_grad_clip_exclusion = True\n",
    "# lambda_penalty = -1\n",
    "# gradient_clip_val = None\n",
    "\n",
    "\n",
    "\n",
    "activation = 'poly'\n",
    "enable_boundary_loss = True\n",
    "disable_batchnorm_grad_clip_exclusion = False\n",
    "lambda_penalty = 10\n",
    "gradient_clip_val = 1.0\n",
    "\n",
    "\n",
    "pol_degree = 2\n",
    "\n",
    "pol_degree_map = {\n",
    "    2:{\"B\": 13, \"penalty_B\": 13},\n",
    "    4:{\"B\": 20, \"penalty_B\": 20 * 0.75},\n",
    "    8:{\"B\": 35, \"penalty_B\": 30 * 0.5},\n",
    "}\n",
    "\n",
    "max_epoch = 10\n",
    "dropout = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "project_name = \"test\"\n",
    "data_workers = 4\n",
    "model = \"mlp\"\n",
    "dataset = {\n",
    "    \"train\": train_set,\n",
    "    \"val\": val_set,\n",
    "    \"test\": test_set\n",
    "}\n",
    "\n",
    "run_id = \"test_mlp\"\n",
    "custom_tag = \"test\"\n",
    "\n",
    "ori_activaiton = \"relu\"\n",
    "samp_size = 100\n",
    "\n",
    "B = pol_degree_map[pol_degree][\"B\"]\n",
    "penalty_B = pol_degree_map[pol_degree][\"penalty_B\"]\n",
    "boundary_loss_params = {'type': 'exp', 'penalty_B':  penalty_B, 'acc_norm': 'sum'}\n",
    "learnable_coeffs = True\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "\n",
    "optimizer_params = {\n",
    "        'type': 'adamw',\n",
    "        'lr': learning_rate,\n",
    "        'params': {\n",
    "        }\n",
    "}\n",
    "scheduler_params = {'type': 'reduce_on_plateau',\n",
    "                        'params': {\n",
    "                                'mode': 'min',\n",
    "                                'factor': 0.1,\n",
    "                                'patience': 5,\n",
    "                                'threshold': 0.1,\n",
    "                                'verbose': True\n",
    "                        },\n",
    "                        'monitor': 'val_acc_epoch'\n",
    "}\n",
    "\n",
    "boundary_loss_params = {'type': 'exp', 'penalty_B': B * 1.0, 'acc_norm': 'sum'}\n",
    "actvation_params =  {\n",
    "        \"ori_activation\": ori_activaiton,\n",
    "        'B': B,\n",
    "        'samp_size': samp_size,\n",
    "        'pol_degree': pol_degree,\n",
    "        'learnable_coeffs': learnable_coeffs,\n",
    "        'initialization': \"least_square\",\n",
    "        'boundary_loss_params': boundary_loss_params\n",
    "\n",
    "    }\n",
    "num_classes = np.unique(y_train).shape[0]\n",
    "model_params = {\n",
    "    \"use_singleton_activation\": False,\n",
    "    \"bn_before_act\": False,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"num_classes\":num_classes,\n",
    "    \"actvation_params\": actvation_params,\n",
    "    \"model\":model,\n",
    "    \"input_size\": input_size,\n",
    "    'hidden_dims': [256, 128, 64, 32, 16]\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"enable_boundary_loss\": enable_boundary_loss,\n",
    "    \"gradient_clip_val\": gradient_clip_val,\n",
    "    \"max_epoch\": max_epoch,\n",
    "    \"lambda_penalty\": lambda_penalty,\n",
    "    \"disable_batchnorm_grad_clip_exclusion\":disable_batchnorm_grad_clip_exclusion,\n",
    "    'optimizer_params': optimizer_params,\n",
    "    'scheduler_params': scheduler_params\n",
    "\n",
    "}\n",
    "\n",
    "dataset_params = {\n",
    "    \"data_workers\": data_workers,\n",
    "    \"dataset\": dataset,\n",
    "    \"batch_size\": 800\n",
    "}\n",
    "\n",
    "project_params = {\"run_id\": run_id,\n",
    "                  \"project_name\": project_name,\n",
    "                  \"custom_tag\": custom_tag\n",
    "                  }\n",
    "\n",
    "run_expriment(project_params=project_params, dataset_params=dataset_params,\n",
    "               model_params=model_params, training_params=training_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
