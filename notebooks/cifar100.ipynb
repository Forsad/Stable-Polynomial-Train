{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "project_root = os.path.abspath('../src')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import wandb\n",
    "key = None\n",
    "if key is not None:\n",
    "    wandb.login(key=key)\n",
    "    os.environ['WANDB_API_KEY'] = key\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"cifar100.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 23 10:30:29 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.08             Driver Version: 550.127.08     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           Off |   00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             58W /  300W |     388MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# train_set = SubsetSC(\"training\")\n",
    "# test_set = SubsetSC(\"testing\")\n",
    "# val_set = SubsetSC(\"validation\")\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "#Load cifar10 dataset\n",
    "# In the dataset loading cell, replace with:\n",
    "\n",
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "transform_train= transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "# Note: In CIFAR100, the test_set and val_set are using the same data\n",
    "# This is because CIFAR100 only comes with train and test splits\n",
    "# For proper evaluation, we should create a validation set from the training data\n",
    "\n",
    "# Create a validation split from the training data\n",
    "\n",
    "# Calculate sizes for train and validation\n",
    "train_set_full = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "total_size = len(train_set_full)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "\n",
    "val_dataset_full = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_test)\n",
    "\n",
    "# Now split both datasets with the same indices\n",
    "generator = torch.Generator().manual_seed(42)  # for reproducibility\n",
    "train_set, _ = torch.utils.data.random_split(train_set_full, [train_size, val_size], generator=generator)\n",
    "generator = torch.Generator().manual_seed(42) \n",
    "_, val_set = torch.utils.data.random_split(val_dataset_full, [train_size, val_size], generator=generator)\n",
    "\n",
    "# The test set remains separate and exclusive\n",
    "test_set = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Now train_set, val_set, and test_set are all exclusive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set), len(test_set), len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from run_experiment import run_expriment\n",
    "import random\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "# activation = 'relu'\n",
    "# enable_boundary_loss = False\n",
    "# disable_batchnorm_grad_clip_exclusion = True\n",
    "# lambda_penalty = -1\n",
    "# gradient_clip_val = None\n",
    "\n",
    "activation = 'poly'\n",
    "enable_boundary_loss = True\n",
    "disable_batchnorm_grad_clip_exclusion = False\n",
    "lambda_penalty = 100\n",
    "gradient_clip_val = 1.0\n",
    "\n",
    "project_name = \"test\"\n",
    "num_classes = 100\n",
    "max_epoch = 30\n",
    "data_workers = 4\n",
    "model = \"resnet18\"\n",
    "dataset = {\"train\": train_set, \"val\": val_set, \"test\": test_set}\n",
    "\n",
    "run_id = \"test\"\n",
    "custom_tag = \"vanilla_exp_B_bn_before_act\"\n",
    "\n",
    "ori_activaiton = \"relu\"\n",
    "samp_size = 100\n",
    "pol_degree = 8\n",
    "dropout=0.2\n",
    "learning_rate = 0.001\n",
    "\n",
    "pol_degree_map = {\n",
    "    2:{\"B\": 13, \"penalty_B\": 13},\n",
    "    4:{\"B\": 30, \"penalty_B\": 30},\n",
    "    8:{\"B\": 35, \"penalty_B\": 35 * 0.7}\n",
    "}\n",
    "\n",
    "B = pol_degree_map[pol_degree][\"B\"]\n",
    "penalty_B = pol_degree_map[pol_degree][\"penalty_B\"]\n",
    "boundary_loss_params = {'type': 'exp', 'penalty_B':  penalty_B, 'acc_norm': 'sum'}\n",
    "learnable_coeffs = True\n",
    "input_size = (3, 32, 32)\n",
    "\n",
    "\n",
    "optimizer_params = {\n",
    "        'type': 'adamw',\n",
    "        'lr': learning_rate,\n",
    "        'params': {\n",
    "        }\n",
    "}\n",
    "\n",
    "scheduler_params = {'type': 'multi_step',\n",
    "                        'params': {\n",
    "                            'milestones': [40, 60, 80, 100],\n",
    "                            'gamma': 0.1\n",
    "                        },\n",
    "                        'monitor': 'val_acc_epoch'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "actvation_params =  {\n",
    "        \"ori_activation\": ori_activaiton,\n",
    "        'B': B,\n",
    "        'samp_size': samp_size,\n",
    "        'pol_degree': pol_degree,\n",
    "        'learnable_coeffs': learnable_coeffs,\n",
    "        'initialization': \"least_square\",\n",
    "        'boundary_loss_params': boundary_loss_params\n",
    "\n",
    "    }\n",
    "model_params = {\n",
    "    \"use_singleton_activation\": False,\n",
    "    \"bn_before_act\": False,\n",
    "    \"activation\": activation,\n",
    "    \"dropout\": dropout,\n",
    "    \"num_classes\":num_classes,\n",
    "    \"actvation_params\": actvation_params,\n",
    "    \"model\":model,\n",
    "    \"input_size\": input_size\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"enable_boundary_loss\": enable_boundary_loss,\n",
    "    \"gradient_clip_val\": gradient_clip_val,\n",
    "    \"max_epoch\": max_epoch,\n",
    "    \"lambda_penalty\": lambda_penalty,\n",
    "    \"disable_batchnorm_grad_clip_exclusion\":disable_batchnorm_grad_clip_exclusion,\n",
    "    'optimizer_params': optimizer_params,\n",
    "    'scheduler_params': scheduler_params\n",
    "\n",
    "}\n",
    "\n",
    "dataset_params = {\n",
    "    \"data_workers\": data_workers,\n",
    "    \"dataset\": dataset,\n",
    "    \"batch_size\": 128\n",
    "}\n",
    "\n",
    "project_params = {\"run_id\": run_id,\n",
    "                  \"project_name\": project_name,\n",
    "                  \"custom_tag\": custom_tag\n",
    "                  }\n",
    "\n",
    "run_expriment(project_params=project_params, dataset_params=dataset_params,\n",
    "               model_params=model_params, training_params=training_params)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hfe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
