# Polynomial Neural Networks for Stable and HE-Compatible Deep Learning

This project implements and explores Polynomial Neural Networks (PNNs), focusing on a robust training framework that enables stable and effective training, particularly for Homomorphic Encryption (HE) applications. The work is based on the research paper: "A Training Framework for Optimal and Stable Training of Polynomial Neural Networks." (https://arxiv.org/abs/2505.11589)

## Project Overview

Polynomial Neural Networks (PNNs), which replace standard non-linearities with polynomial activations, are pivotal for applications such as privacy-preserving inference via Homomorphic Encryption (HE). This is because their core operations (additions and multiplications) align well with the capabilities of current HE schemes.

However, training PNNs effectively presents a significant challenge:
* Low-degree polynomials can limit model expressivity.
* Higher-degree polynomials, crucial for capturing complex functions, often suffer from numerical instability and gradient explosion

This project implements a novel training framework designed to overcome these obstacles.

## Key Features & Contributions (from the research)

This codebase implements the training framework described in the accompanying paper, which features two synergistic innovations:

1.  **Boundary Loss:** A novel mechanism that exponentially penalizes activation inputs outside a predefined stable range (e.g., [-B, B]), preventing divergence. The composite loss function combines cross-entropy loss with this boundary loss.
2.  **Selective Gradient Clipping:** This technique effectively tames gradient magnitudes while preserving essential Batch Normalization statistics by excluding its parameters (γ and β) from the clipping process

This framework enables:
* **Stable Training:** Successfully train PNNs with polynomial degrees up to 22, where standard methods typically fail
* **High Accuracy:** Achieve performance comparable to, and in some cases matching or surpassing, their original ReLU-based counterparts, even with low-degree (e.g., degree 2) polynomial activations
* **HE Compatibility:** The trained models are designed to be HE-compatible, using layers like linear layers, average pooling, and batch normalization (fused during inference)
* **Broad Efficacy:** Demonstrated across diverse image (MNIST, FashionMNIST, CIFAR-10, CIFAR-100), audio (Speech Commands), and human activity recognition (UCI-HAR, Capture24) datasets.

## Project Structure

* `src/`: Contains the core Python modules for models (ResNet-18, MLP), activations (polynomial fitting using least squares), losses (CustomPolyLoss including boundary penalty), and training logic (PyTorch Lightning based).
* `notebooks/`: Contains Jupyter notebooks for experiments, examples, and demonstrations across the various datasets mentioned.
* `notebooks/data/`: Directory for datasets (typically gitignored, see setup instructions if needed).
* `hfe.yml`: Conda environment file for reproducing the development environment.

## Installation

### 1. Set Up Conda Environment

It is recommended to use Conda to manage dependencies for this project. The `hfe.yml` file specifies the environment.

1.  **Ensure Conda is installed.** If not, follow the installation instructions for [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/products/distribution).

2.  **Create the Conda environment from the `hfe.yml` file:**
    ```bash
    conda env create -f hfe.yml
    ```

3.  **Activate the environment:**
    ```bash
    conda activate hfe
    ```

## Dataset Setup

* For **CIFAR-10, CIFAR-100, MNIST, FashionMNIST, Speech Commands**, the project uses the PyTorch built-in dataset classes, which will be fetched automatically.
    * For image datasets, training involves standard data augmentation like random horizontal flips and rotations.
    * Speech Commands audio waveforms are converted to Mel spectrograms and normalized.
* For the **uci-har dataset and capture24 dataset**, please download the data from this [link](https://drive.google.com/drive/folders/1xx8yHiNH1iB2VyA2yACqxV-E5JXm0ZLZ?usp=share_link) if you don't want to download and process them manually. The paper details specific preprocessing for UCI-HAR (subject-based train/validation split, normalization)  and Capture-24 (subject-based train/test/validation splits, normalization).

Your final directory structure for data should look like the following (as generated by the notebooks):

notebooks/data/
- speech_commands/
- FashionMNIST/
- capture24/
- uci_har/
- MNIST/
- cifar-100-python/
- cifar-10-batches-py/

## Exploring Notebooks

Example notebooks demonstrating various aspects of the project, including dataset loading, model training (using the `run_experiment.py` script), and analysis, can be found in the `notebooks/` directory. These notebooks cover experiments on:
* MNIST
* FashionMNIST
* CIFAR-10
* CIFAR-100
* Speech Commands
* UCI-HAR
* Capture24

Please navigate to the `notebooks/` folder to open and run the examples.

## Citation

This project implements the framework presented in the following paper:

*Forsad Al Hossain and Tauhidur Rahman. "A Training Framework for Optimal and Stable Training of Polynomial Neural Networks." (https://arxiv.org/abs/2505.11589)

If you use this code in your research, please consider citing the paper

```bibtex
@misc{hossain2025trainingframeworkoptimalstable,
      title={A Training Framework for Optimal and Stable Training of Polynomial Neural Networks}, 
      author={Forsad Al Hossain and Tauhidur Rahman},
      year={2025},
      eprint={2505.11589},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={[https://arxiv.org/abs/2505.11589](https://arxiv.org/abs/2505.11589)}, 
}